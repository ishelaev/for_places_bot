import re
import time
import random
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By

day_map = {
    "Mo": "–ü–Ω",
    "Tu": "–í—Ç",
    "We": "–°—Ä",
    "Th": "–ß—Ç",
    "Fr": "–ü—Ç",
    "Sa": "–°–±",
    "Su": "–í—Å"
}

def get_review_form(count):
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—Ä–∞–≤–∏–ª—å–Ω—É—é —Ñ–æ—Ä–º—É —Å–ª–æ–≤–∞ '–æ—Ç–∑—ã–≤' –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —á–∏—Å–ª–∞"""
    count = int(count)
    if count % 10 == 1 and count % 100 != 11:
        return f"{count} –æ—Ç–∑—ã–≤"
    elif 2 <= count % 10 <= 4 and (count % 100 < 10 or count % 100 >= 20):
        return f"{count} –æ—Ç–∑—ã–≤–∞"
    else:
        return f"{count} –æ—Ç–∑—ã–≤–æ–≤"

def parse_yandex_requests(url: str) -> dict:
    """–ü–∞—Ä—Å–∏–Ω–≥ —á–µ—Ä–µ–∑ requests (–±—ã—Å—Ç—Ä–æ, –Ω–æ –º–æ–∂–µ—Ç –Ω–µ —Å—Ä–∞–±–æ—Ç–∞—Ç—å)"""
    try:
        import requests
        headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.2.1 Safari/605.1.15',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'ru-RU,ru;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        }
        
        response = requests.get(url, headers=headers, timeout=10)
        soup = BeautifulSoup(response.text, "html.parser")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –∫–∞–ø—á—É
        title_tag = soup.find('h1')
        title = title_tag.text.strip() if title_tag else ""
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –∫–∞–ø—á—É (–±–æ–ª–µ–µ —Ç–æ—á–Ω–∞—è –¥–µ—Ç–µ–∫—Ü–∏—è)
        if "√∞¬æ√∞¬¥√±√∞¬≤√∞" in title or "√±√∞¬æ√∞¬±√∞¬æ√±" in title or len(title) > 50:
            return None  # –ö–∞–ø—á–∞ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∞
        
        # –ï—Å–ª–∏ –≤—Å—ë –æ–∫ - –ø–∞—Ä—Å–∏–º –í–°–ï –¥–∞–Ω–Ω—ã–µ –∫–∞–∫ –≤ Selenium –≤–µ—Ä—Å–∏–∏
        
        # ===== –†–µ–π—Ç–∏–Ω–≥ =====
        rating = None
        rating_tag = soup.select_one("span.business-rating-badge-view__rating-text")
        if rating_tag:
            rating = rating_tag.get_text(strip=True)
        
        # ===== –û—Ç–∑—ã–≤—ã =====
        reviews = None
        reviews_tag = soup.select_one("meta[itemprop='reviewCount']")
        if reviews_tag:
            reviews = reviews_tag.get("content")
            if reviews:
                reviews = get_review_form(reviews)
        
        # ===== –ö–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã =====
        latitude = longitude = None
        el = soup.select_one('[data-coordinates]')
        if el and el.get('data-coordinates'):
            try:
                lon_str, lat_str = el['data-coordinates'].split(',', 1)
                longitude = float(lon_str.strip())
                latitude = float(lat_str.strip())
            except Exception:
                pass
        
        # fallback —á–µ—Ä–µ–∑ regex –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ
        if latitude is None or longitude is None:
            m = re.search(r'data-coordinates="([\-0-9\.]+),([\-0-9\.]+)"', response.text)
            if m:
                try:
                    longitude = float(m.group(1))
                    latitude = float(m.group(2))
                except Exception:
                    pass
        
        # ===== –ß–∞—Å—ã —Ä–∞–±–æ—Ç—ã =====
        hours = {}
        hours_tags = soup.select("meta[itemprop='openingHours']")
        for tag in hours_tags:
            content = tag.get("content")
            if not content:
                continue
            match = re.match(r"([A-Za-z]{2}) (.+)", content)
            if match:
                eng_day, time_range = match.groups()
                ru_day = day_map.get(eng_day, eng_day)
                hours[ru_day] = time_range
        
        # ===== –ö–∞—Ç–µ–≥–æ—Ä–∏–∏ =====
        categories = None
        categories_tags = soup.select("a.orgpage-categories-info-view__link span.button__text")
        if categories_tags:
            parts = [tag.get_text(strip=True) for tag in categories_tags if tag.get_text(strip=True)]
            if parts:
                categories = ", ".join(parts)
        
        result = {
            "title": title or "–ù–∞–∑–≤–∞–Ω–∏–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ",
            "rating": rating or "–†–µ–π—Ç–∏–Ω–≥ –Ω–µ –Ω–∞–π–¥–µ–Ω",
            "reviews": reviews or "–û—Ç–∑—ã–≤—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã",
            "coordinates": (latitude, longitude) if latitude is not None and longitude is not None else None,
            "hours": hours,
            "categories": categories
        }
        return result
        
    except Exception:
        return None

def parse_yandex_selenium(url: str) -> dict:
    # --- –ù–∞—Å—Ç—Ä–æ–π–∫–∞ Selenium ---
    options = Options()
    options.add_argument("--headless")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    
    # –ú–∞—Å–∫–∏—Ä–æ–≤–∫–∞ –ø–æ–¥ –æ–±—ã—á–Ω—ã–π –±—Ä–∞—É–∑–µ—Ä
    options.add_argument("--disable-blink-features=AutomationControlled")
    options.add_argument("--user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")
    options.add_argument(f"--remote-debugging-port={random.randint(9000, 9999)}")
    options.add_argument("--disable-extensions")
    options.add_argument("--disable-plugins")
    
    # –û—Ç–∫–ª—é—á–∞–µ–º webdriver —Ä–µ–∂–∏–º
    options.add_experimental_option("excludeSwitches", ["enable-automation"])
    options.add_experimental_option('useAutomationExtension', False)

    driver = webdriver.Chrome(options=options)
    driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
    
    # –ò–º–∏—Ç–∏—Ä—É–µ–º —á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–µ –ø–æ–≤–µ–¥–µ–Ω–∏–µ
    driver.get(url)
    time.sleep(random.uniform(3, 6))  # —Å–ª—É—á–∞–π–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ 3-6 —Å–µ–∫

    # ===== –ù–∞–∑–≤–∞–Ω–∏–µ =====
    title = None
    try:
        title = driver.find_element(By.TAG_NAME, "h1").text.strip()
    except:
        pass

    # –ü–æ–ª—É—á–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É –¥–ª—è BeautifulSoup
    soup = BeautifulSoup(driver.page_source, "html.parser")

    # ===== –†–µ–π—Ç–∏–Ω–≥ =====
    rating = None
    rating_tag = soup.select_one("span.business-rating-badge-view__rating-text")
    if rating_tag:
        rating = rating_tag.get_text(strip=True)

    # ===== –û—Ç–∑—ã–≤—ã =====
    reviews = None
    reviews_tag = soup.select_one("meta[itemprop='reviewCount']")
    if reviews_tag:
        reviews = reviews_tag.get("content")
        if reviews:
            reviews = get_review_form(reviews)

    # ===== –ö–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã (lon,lat ‚Üí lat,lon float) =====
    latitude = longitude = None
    el = soup.select_one('[data-coordinates]')
    if el and el.get('data-coordinates'):
        try:
            lon_str, lat_str = el['data-coordinates'].split(',', 1)
            longitude = float(lon_str.strip())
            latitude = float(lat_str.strip())
        except Exception:
            pass

    # fallback —á–µ—Ä–µ–∑ regex –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ
    if latitude is None or longitude is None:
        m = re.search(r'data-coordinates="([\-0-9\.]+),([\-0-9\.]+)"', driver.page_source)
        if m:
            try:
                longitude = float(m.group(1))
                latitude = float(m.group(2))
            except Exception:
                pass

    # fallback —á–µ—Ä–µ–∑ URL
    if latitude is None or longitude is None:
        m = re.search(r'[?&]ll=([\-0-9\.]+),([\-0-9\.]+)', driver.current_url)
        if m:
            try:
                longitude = float(m.group(1))
                latitude = float(m.group(2))
            except Exception:
                pass

    # ===== –ß–∞—Å—ã —Ä–∞–±–æ—Ç—ã =====
    hours = {}
    hours_tags = soup.select("meta[itemprop='openingHours']")
    for tag in hours_tags:
        content = tag.get("content")
        if not content:
            continue
        match = re.match(r"([A-Za-z]{2}) (.+)", content)
        if match:
            eng_day, time_range = match.groups()
            ru_day = day_map.get(eng_day, eng_day)
            hours[ru_day] = time_range

    # ===== –ö–∞—Ç–µ–≥–æ—Ä–∏–∏ =====
    categories = None
    categories_tags = soup.select("a.orgpage-categories-info-view__link span.button__text")
    if categories_tags:
        parts = [tag.get_text(strip=True) for tag in categories_tags if tag.get_text(strip=True)]
        if parts:
            categories = ", ".join(parts)

    driver.quit()
    
    result = {
        "title": title or "–ù–∞–∑–≤–∞–Ω–∏–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ",
        "rating": rating or "–†–µ–π—Ç–∏–Ω–≥ –Ω–µ –Ω–∞–π–¥–µ–Ω",
        "reviews": reviews or "–û—Ç–∑—ã–≤—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã",
        "coordinates": (latitude, longitude) if latitude is not None and longitude is not None else None,
        "hours": hours,
        "categories": categories
    }
    
    return result

def parse_yandex(url: str) -> dict:
    """
    –£–º–Ω—ã–π –ø–∞—Ä—Å–µ—Ä —Å fallback –º–µ—Ç–æ–¥–∞–º–∏
    1. –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ—Ç –±—ã—Å—Ç—Ä—ã–π requests
    2. –ï—Å–ª–∏ –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç - –∏—Å–ø–æ–ª—å–∑—É–µ—Ç Selenium
    """
    print("üîÑ –ü—Ä–æ–±—É—é –±—ã—Å—Ç—Ä—ã–π –º–µ—Ç–æ–¥ (requests)...")
    result = parse_yandex_requests(url)
    
    if result is not None:
        print("‚úÖ Requests —Å—Ä–∞–±–æ—Ç–∞–ª!")
        return result
    
    print("üîÑ Requests –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª, –ø—Ä–æ–±—É—é Selenium...")
    return parse_yandex_selenium(url)
